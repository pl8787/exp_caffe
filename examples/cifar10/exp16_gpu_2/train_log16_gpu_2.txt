
D:\Scratch\lipang\cifar10\examples\cifar10\exp16_gpu_2>set TOOLS=..\..\..\build\tools 

D:\Scratch\lipang\cifar10\examples\cifar10\exp16_gpu_2>set GLOG_logtostderr=1 

D:\Scratch\lipang\cifar10\examples\cifar10\exp16_gpu_2>..\..\..\build\tools\train_net.exe cifar10_quick_solver_co.prototxt 
I0927 18:57:52.308902 12876 train_net.cpp:26] Starting Optimization
I0927 18:57:52.308902 12876 solver.cpp:41] Creating training net.
I0927 18:57:52.308902 12876 net.cpp:75] Creating Layer cifar
I0927 18:57:52.308902 12876 net.cpp:111] cifar -> data
I0927 18:57:52.308902 12876 net.cpp:111] cifar -> label
I0927 18:57:52.308902 12876 data_layer.cpp:145] Opening leveldb ../cifar10-leveldb/cifar-train-leveldb
I0927 18:57:52.308902 12876 data_layer.cpp:185] output data size: 100,3,32,32
I0927 18:57:52.308902 12876 data_layer.cpp:204] Loading mean file from../mean.binaryproto
I0927 18:57:53.137073 12876 net.cpp:126] Top shape: 100 3 32 32 (307200)
I0927 18:57:53.137073 12876 net.cpp:126] Top shape: 100 1 1 1 (100)
I0927 18:57:53.137073 12876 net.cpp:157] cifar does not need backward computation.
I0927 18:57:53.137073 12876 net.cpp:75] Creating Layer conv1
I0927 18:57:53.137073 12876 net.cpp:85] conv1 <- data
I0927 18:57:53.137073 12876 net.cpp:111] conv1 -> conv1
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 16 32 32 (1638400)
I0927 18:57:53.152699 12876 net.cpp:152] conv1 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer pool1
I0927 18:57:53.152699 12876 net.cpp:85] pool1 <- conv1
I0927 18:57:53.152699 12876 net.cpp:111] pool1 -> pool1
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0927 18:57:53.152699 12876 net.cpp:152] pool1 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer relu1
I0927 18:57:53.152699 12876 net.cpp:85] relu1 <- pool1
I0927 18:57:53.152699 12876 net.cpp:99] relu1 -> pool1 (in-place)
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0927 18:57:53.152699 12876 net.cpp:152] relu1 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer conv2
I0927 18:57:53.152699 12876 net.cpp:85] conv2 <- pool1
I0927 18:57:53.152699 12876 net.cpp:111] conv2 -> conv2
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0927 18:57:53.152699 12876 net.cpp:152] conv2 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer relu2
I0927 18:57:53.152699 12876 net.cpp:85] relu2 <- conv2
I0927 18:57:53.152699 12876 net.cpp:99] relu2 -> conv2 (in-place)
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0927 18:57:53.152699 12876 net.cpp:152] relu2 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer pool2
I0927 18:57:53.152699 12876 net.cpp:85] pool2 <- conv2
I0927 18:57:53.152699 12876 net.cpp:111] pool2 -> pool2
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 32 8 8 (204800)
I0927 18:57:53.152699 12876 net.cpp:152] pool2 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer conv3
I0927 18:57:53.152699 12876 net.cpp:85] conv3 <- pool2
I0927 18:57:53.152699 12876 net.cpp:111] conv3 -> conv3
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0927 18:57:53.152699 12876 net.cpp:152] conv3 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer relu3
I0927 18:57:53.152699 12876 net.cpp:85] relu3 <- conv3
I0927 18:57:53.152699 12876 net.cpp:99] relu3 -> conv3 (in-place)
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0927 18:57:53.152699 12876 net.cpp:152] relu3 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer pool3
I0927 18:57:53.152699 12876 net.cpp:85] pool3 <- conv3
I0927 18:57:53.152699 12876 net.cpp:111] pool3 -> pool3
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 64 4 4 (102400)
I0927 18:57:53.152699 12876 net.cpp:152] pool3 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer ip1
I0927 18:57:53.152699 12876 net.cpp:85] ip1 <- pool3
I0927 18:57:53.152699 12876 net.cpp:111] ip1 -> ip1
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 64 1 1 (6400)
I0927 18:57:53.152699 12876 net.cpp:152] ip1 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer ip2
I0927 18:57:53.152699 12876 net.cpp:85] ip2 <- ip1
I0927 18:57:53.152699 12876 net.cpp:111] ip2 -> ip2
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 10 1 1 (1000)
I0927 18:57:53.152699 12876 net.cpp:152] ip2 needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer loss
I0927 18:57:53.152699 12876 net.cpp:85] loss <- ip2
I0927 18:57:53.152699 12876 net.cpp:85] loss <- label
I0927 18:57:53.152699 12876 net.cpp:152] loss needs backward computation.
I0927 18:57:53.152699 12876 net.cpp:181] Collecting Learning Rate and Weight Decay.
I0927 18:57:53.152699 12876 net.cpp:174] Network initialization done.
I0927 18:57:53.152699 12876 net.cpp:175] Memory required for Data 15594800
I0927 18:57:53.152699 12876 solver.cpp:44] Creating testing net.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer cifar
I0927 18:57:53.152699 12876 net.cpp:111] cifar -> data
I0927 18:57:53.152699 12876 net.cpp:111] cifar -> label
I0927 18:57:53.152699 12876 data_layer.cpp:145] Opening leveldb ../cifar10-leveldb/cifar-test-leveldb
I0927 18:57:53.152699 12876 data_layer.cpp:185] output data size: 100,3,32,32
I0927 18:57:53.152699 12876 data_layer.cpp:204] Loading mean file from../mean.binaryproto
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 3 32 32 (307200)
I0927 18:57:53.152699 12876 net.cpp:126] Top shape: 100 1 1 1 (100)
I0927 18:57:53.152699 12876 net.cpp:157] cifar does not need backward computation.
I0927 18:57:53.152699 12876 net.cpp:75] Creating Layer conv1
I0927 18:57:53.152699 12876 net.cpp:85] conv1 <- data
I0927 18:57:53.152699 12876 net.cpp:111] conv1 -> conv1
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 16 32 32 (1638400)
I0927 18:57:53.168325 12876 net.cpp:152] conv1 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer pool1
I0927 18:57:53.168325 12876 net.cpp:85] pool1 <- conv1
I0927 18:57:53.168325 12876 net.cpp:111] pool1 -> pool1
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0927 18:57:53.168325 12876 net.cpp:152] pool1 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer relu1
I0927 18:57:53.168325 12876 net.cpp:85] relu1 <- pool1
I0927 18:57:53.168325 12876 net.cpp:99] relu1 -> pool1 (in-place)
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0927 18:57:53.168325 12876 net.cpp:152] relu1 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer conv2
I0927 18:57:53.168325 12876 net.cpp:85] conv2 <- pool1
I0927 18:57:53.168325 12876 net.cpp:111] conv2 -> conv2
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0927 18:57:53.168325 12876 net.cpp:152] conv2 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer relu2
I0927 18:57:53.168325 12876 net.cpp:85] relu2 <- conv2
I0927 18:57:53.168325 12876 net.cpp:99] relu2 -> conv2 (in-place)
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0927 18:57:53.168325 12876 net.cpp:152] relu2 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer pool2
I0927 18:57:53.168325 12876 net.cpp:85] pool2 <- conv2
I0927 18:57:53.168325 12876 net.cpp:111] pool2 -> pool2
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 32 8 8 (204800)
I0927 18:57:53.168325 12876 net.cpp:152] pool2 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer conv3
I0927 18:57:53.168325 12876 net.cpp:85] conv3 <- pool2
I0927 18:57:53.168325 12876 net.cpp:111] conv3 -> conv3
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0927 18:57:53.168325 12876 net.cpp:152] conv3 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer relu3
I0927 18:57:53.168325 12876 net.cpp:85] relu3 <- conv3
I0927 18:57:53.168325 12876 net.cpp:99] relu3 -> conv3 (in-place)
I0927 18:57:53.168325 12876 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0927 18:57:53.168325 12876 net.cpp:152] relu3 needs backward computation.
I0927 18:57:53.168325 12876 net.cpp:75] Creating Layer pool3
I0927 18:57:53.
D:\Scratch\lipang\cifar10\examples\cifar10\exp16_gpu_2>#reduce learning rate by fctor of 10 after 8 epochs 
'#reduce' is not recognized as an internal or external command,
operable program or batch file.

D:\Scratch\lipang\cifar10\examples\cifar10\exp16_gpu_2>..\..\..\build\tools\train_net.exe cifar10_quick_solver_lr1_co.prototxt cifar10_quick_co_iter_4000.solverstate 
I0928 09:35:10.873029  2724 train_net.cpp:26] Starting Optimization
I0928 09:35:10.873029  2724 solver.cpp:41] Creating training net.
I0928 09:35:10.873029  2724 net.cpp:75] Creating Layer cifar
I0928 09:35:10.873029  2724 net.cpp:111] cifar -> data
I0928 09:35:10.873029  2724 net.cpp:111] cifar -> label
I0928 09:35:10.873029  2724 data_layer.cpp:145] Opening leveldb ../cifar10-leveldb/cifar-train-leveldb
I0928 09:35:10.873029  2724 data_layer.cpp:185] output data size: 100,3,32,32
I0928 09:35:10.873029  2724 data_layer.cpp:204] Loading mean file from../mean.binaryproto
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 3 32 32 (307200)
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 1 1 1 (100)
I0928 09:35:11.701203  2724 net.cpp:157] cifar does not need backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer conv1
I0928 09:35:11.701203  2724 net.cpp:85] conv1 <- data
I0928 09:35:11.701203  2724 net.cpp:111] conv1 -> conv1
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 16 32 32 (1638400)
I0928 09:35:11.701203  2724 net.cpp:152] conv1 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer pool1
I0928 09:35:11.701203  2724 net.cpp:85] pool1 <- conv1
I0928 09:35:11.701203  2724 net.cpp:111] pool1 -> pool1
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0928 09:35:11.701203  2724 net.cpp:152] pool1 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer relu1
I0928 09:35:11.701203  2724 net.cpp:85] relu1 <- pool1
I0928 09:35:11.701203  2724 net.cpp:99] relu1 -> pool1 (in-place)
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0928 09:35:11.701203  2724 net.cpp:152] relu1 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer conv2
I0928 09:35:11.701203  2724 net.cpp:85] conv2 <- pool1
I0928 09:35:11.701203  2724 net.cpp:111] conv2 -> conv2
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0928 09:35:11.701203  2724 net.cpp:152] conv2 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer relu2
I0928 09:35:11.701203  2724 net.cpp:85] relu2 <- conv2
I0928 09:35:11.701203  2724 net.cpp:99] relu2 -> conv2 (in-place)
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0928 09:35:11.701203  2724 net.cpp:152] relu2 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer pool2
I0928 09:35:11.701203  2724 net.cpp:85] pool2 <- conv2
I0928 09:35:11.701203  2724 net.cpp:111] pool2 -> pool2
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 32 8 8 (204800)
I0928 09:35:11.701203  2724 net.cpp:152] pool2 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer conv3
I0928 09:35:11.701203  2724 net.cpp:85] conv3 <- pool2
I0928 09:35:11.701203  2724 net.cpp:111] conv3 -> conv3
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0928 09:35:11.701203  2724 net.cpp:152] conv3 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer relu3
I0928 09:35:11.701203  2724 net.cpp:85] relu3 <- conv3
I0928 09:35:11.701203  2724 net.cpp:99] relu3 -> conv3 (in-place)
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0928 09:35:11.701203  2724 net.cpp:152] relu3 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer pool3
I0928 09:35:11.701203  2724 net.cpp:85] pool3 <- conv3
I0928 09:35:11.701203  2724 net.cpp:111] pool3 -> pool3
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 64 4 4 (102400)
I0928 09:35:11.701203  2724 net.cpp:152] pool3 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer ip1
I0928 09:35:11.701203  2724 net.cpp:85] ip1 <- pool3
I0928 09:35:11.701203  2724 net.cpp:111] ip1 -> ip1
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 64 1 1 (6400)
I0928 09:35:11.701203  2724 net.cpp:152] ip1 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer ip2
I0928 09:35:11.701203  2724 net.cpp:85] ip2 <- ip1
I0928 09:35:11.701203  2724 net.cpp:111] ip2 -> ip2
I0928 09:35:11.701203  2724 net.cpp:126] Top shape: 100 10 1 1 (1000)
I0928 09:35:11.701203  2724 net.cpp:152] ip2 needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer loss
I0928 09:35:11.701203  2724 net.cpp:85] loss <- ip2
I0928 09:35:11.701203  2724 net.cpp:85] loss <- label
I0928 09:35:11.701203  2724 net.cpp:152] loss needs backward computation.
I0928 09:35:11.701203  2724 net.cpp:181] Collecting Learning Rate and Weight Decay.
I0928 09:35:11.701203  2724 net.cpp:174] Network initialization done.
I0928 09:35:11.701203  2724 net.cpp:175] Memory required for Data 15594800
I0928 09:35:11.701203  2724 solver.cpp:44] Creating testing net.
I0928 09:35:11.701203  2724 net.cpp:75] Creating Layer cifar
I0928 09:35:11.701203  2724 net.cpp:111] cifar -> data
I0928 09:35:11.701203  2724 net.cpp:111] cifar -> label
I0928 09:35:11.701203  2724 data_layer.cpp:145] Opening leveldb ../cifar10-leveldb/cifar-test-leveldb
I0928 09:35:11.701203  2724 data_layer.cpp:185] output data size: 100,3,32,32
I0928 09:35:11.701203  2724 data_layer.cpp:204] Loading mean file from../mean.binaryproto
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 3 32 32 (307200)
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 1 1 1 (100)
I0928 09:35:11.716830  2724 net.cpp:157] cifar does not need backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer conv1
I0928 09:35:11.716830  2724 net.cpp:85] conv1 <- data
I0928 09:35:11.716830  2724 net.cpp:111] conv1 -> conv1
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 16 32 32 (1638400)
I0928 09:35:11.716830  2724 net.cpp:152] conv1 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer pool1
I0928 09:35:11.716830  2724 net.cpp:85] pool1 <- conv1
I0928 09:35:11.716830  2724 net.cpp:111] pool1 -> pool1
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0928 09:35:11.716830  2724 net.cpp:152] pool1 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer relu1
I0928 09:35:11.716830  2724 net.cpp:85] relu1 <- pool1
I0928 09:35:11.716830  2724 net.cpp:99] relu1 -> pool1 (in-place)
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 16 16 16 (409600)
I0928 09:35:11.716830  2724 net.cpp:152] relu1 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer conv2
I0928 09:35:11.716830  2724 net.cpp:85] conv2 <- pool1
I0928 09:35:11.716830  2724 net.cpp:111] conv2 -> conv2
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0928 09:35:11.716830  2724 net.cpp:152] conv2 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer relu2
I0928 09:35:11.716830  2724 net.cpp:85] relu2 <- conv2
I0928 09:35:11.716830  2724 net.cpp:99] relu2 -> conv2 (in-place)
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 32 16 16 (819200)
I0928 09:35:11.716830  2724 net.cpp:152] relu2 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer pool2
I0928 09:35:11.716830  2724 net.cpp:85] pool2 <- conv2
I0928 09:35:11.716830  2724 net.cpp:111] pool2 -> pool2
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 32 8 8 (204800)
I0928 09:35:11.716830  2724 net.cpp:152] pool2 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer conv3
I0928 09:35:11.716830  2724 net.cpp:85] conv3 <- pool2
I0928 09:35:11.716830  2724 net.cpp:111] conv3 -> conv3
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0928 09:35:11.716830  2724 net.cpp:152] conv3 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer relu3
I0928 09:35:11.716830  2724 net.cpp:85] relu3 <- conv3
I0928 09:35:11.716830  2724 net.cpp:99] relu3 -> conv3 (in-place)
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 64 8 8 (409600)
I0928 09:35:11.716830  2724 net.cpp:152] relu3 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer pool3
I0928 09:35:11.716830  2724 net.cpp:85] pool3 <- conv3
I0928 09:35:11.716830  2724 net.cpp:111] pool3 -> pool3
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 64 4 4 (102400)
I0928 09:35:11.716830  2724 net.cpp:152] pool3 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer ip1
I0928 09:35:11.716830  2724 net.cpp:85] ip1 <- pool3
I0928 09:35:11.716830  2724 net.cpp:111] ip1 -> ip1
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 64 1 1 (6400)
I0928 09:35:11.716830  2724 net.cpp:152] ip1 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer ip2
I0928 09:35:11.716830  2724 net.cpp:85] ip2 <- ip1
I0928 09:35:11.716830  2724 net.cpp:111] ip2 -> ip2
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 10 1 1 (1000)
I0928 09:35:11.716830  2724 net.cpp:152] ip2 needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer prob
I0928 09:35:11.716830  2724 net.cpp:85] prob <- ip2
I0928 09:35:11.716830  2724 net.cpp:111] prob -> prob
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 100 10 1 1 (1000)
I0928 09:35:11.716830  2724 net.cpp:152] prob needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:75] Creating Layer accuracy
I0928 09:35:11.716830  2724 net.cpp:85] accuracy <- prob
I0928 09:35:11.716830  2724 net.cpp:85] accuracy <- label
I0928 09:35:11.716830  2724 net.cpp:111] accuracy -> accuracy
I0928 09:35:11.716830  2724 net.cpp:126] Top shape: 1 2 1 1 (2)
I0928 09:35:11.716830  2724 net.cpp:152] accuracy needs backward computation.
I0928 09:35:11.716830  2724 net.cpp:163] This network produces output accuracy
I0928 09:35:11.716830  2724 net.cpp:181] Collecting Learning Rate and Weight Decay.
I0928 09:35:11.716830  2724 net.cpp:174] Network initialization done.
I0928 09:35:11.716830  2724 net.cpp:175] Memory required for Data 15598808
I0928 09:35:11.716830  2724 solver.cpp:49] Solver scaffolding done.
I0928 09:35:11.716830  2724 train_net.cpp:29] Resuming from cifar10_quick_co_iter_4000.solverstate
^CTerminate batch job (Y/N)? 